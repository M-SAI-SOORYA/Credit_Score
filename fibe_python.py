# -*- coding: utf-8 -*-
"""fibe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wPb5RHqxpBHkwMmnsTeXwXSCng9_PvMV
"""


import pickle
import pandas as pd
df=pd.read_csv('train.csv')

df.head()

df.shape

df=df.drop('id',axis=1)

import numpy as np
print(np.any(np.isnan(df)))
print(np.any(np.isinf(df)))

# dataset contains high extreme or low extreme values
# replacing infinity values with some large value or small values
df.replace([np.inf,-np.inf],-1e9,inplace=True)

print(np.any(np.isinf(df)))

percent_missing = df.isnull().sum() * 100 / len(df)
missing_value_df = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})
missing_value_df = missing_value_df.sort_values('percent_missing', ascending=False)
missing_value_df

mxmiss=missing_value_df[missing_value_df['percent_missing']>=80.0]

mxmiss.shape

# so we've 1861 attributes which have more than 80 percent missing values, we need to drop them out
row_index=mxmiss.index
row_index=list(row_index)
df=df.drop(row_index,axis=1)

df.info()

# now we've removed  coloumns which have more than 80% missing values

df.head()

print(np.any(np.isnan(df))) # true indicates it has nan values

dff=df.iloc[:,:-1]
dff
y=df.iloc[:,-1]

dff.head()

dff.to_csv('data.csv')

import dask.dataframe as dd 
df_da=dd.read_csv('data.csv') # reading the dataframe as dask_dataframe to make it into chunks and preprocessing ( any process ) will be easy

def dd(df):
  df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
  return df
df_da=dd(df_da)

df_da.head()

# converting into numpy array
def mgen(df):
  data=df.values
  return data
df_dask1=mgen(df_da)

df_dask1

# replacing all nan values with zero
df_dask1[np.isnan(df_dask1)]=0

type(df_dask1)

def col(df):
  columns=df.columns
  return columns
columns=col(df_da)

len(columns)

# standardizing the data ( scaling the data )
from sklearn.preprocessing import MinMaxScaler
def minmax(df_df):
  global scaler
  scaler=MinMaxScaler()
  dd=scaler.fit_transform(df_df)
  return dd
data_scaled=minmax(df_dask1)

data_scaled

data_scaled.shape[1]

"""
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt

n_components = (data_scaled.shape[1])
svd = TruncatedSVD(n_components=n_components)
svd.fit_transform(data_scaled) 
explained_var_ratio = svd.explained_variance_ratio_
plt.plot(np.cumsum(explained_var_ratio))
plt.show()
"""

# as we can observe from the above scree plot the elbow taking place at the range(0-500)
# take n_components as 500

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=500)
data_imputed=svd.fit_transform(data_scaled)

data_imputed.shape

svd.components_.shape

data_imp=np.dot(data_imputed,svd.components_)

data_imp.shape

# descaling the data
final_data=scaler.inverse_transform(data_imp)

final_data

final_data.shape

# converting it into a dataframe
final_dataframe=pd.DataFrame(final_data,columns=columns)

final_dataframe.head()

class_distribution=pd.value_counts(y,normalize=True)
print(class_distribution) # therefore, the dataset is imbalanced

# we perform undersampling to reduce the instances in the majority class to class distribution
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(final_dataframe, y)

X_resampled.head()

class_distribution1=pd.value_counts(y_resampled,normalize=True)
print(class_distribution1) # therefore, the dataset is balanced

print(np.any(np.isnan(X_resampled))) # no nan values

df_test=pd.read_csv('/content/drive/MyDrive/fibe hackathon/dataset/test.csv')
df_test.head()

df_test=df_test.drop('id',axis=1)

df_test.replace([np.inf,-np.inf],-1e9,inplace=True)

print(np.any(np.isinf(df_test)))

import numpy as np

percent_missing_test1 = df_test.isnull().sum() * 100 / len(df_test)
missing_value_df_test1 = pd.DataFrame({'column_name': df_test.columns,'percent_missing': percent_missing_test1})
missing_value_df_test2= missing_value_df_test1.sort_values('percent_missing', ascending=False)
missing_value_df_test2

mxmiss_test1=missing_value_df_test1[missing_value_df_test1['percent_missing']>=80.0]

mxmiss_test1

mxmiss_test1.shape

row_index_test=mxmiss_test1.index
row_index_test=list(row_index_test)
df_test=df_test.drop(row_index_test,axis=1)

df_test.head()

print(np.any(np.isinf(df_test)))
print(np.any(np.isnan(df_test)))

df_test.to_csv('df_test.csv')

import dask.dataframe as dd

df_test_dd=dd.read_csv('df_test.csv')

df_test_dd.head()

def dd(df):
  df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
  return df
df_daa=dd(df_test_dd)

# converting into numpy array
def mgen(df):
  data=df.values
  return data
df_dask2=mgen(df_daa)

# replacing all nan values with zero
df_dask2[np.isnan(df_dask2)]=0

type(df_dask2)

def col(df):
  columns=df.columns
  return columns
columns1=col(df_daa)

len(columns1)

# standardizing the data ( scaling the data )
from sklearn.preprocessing import MinMaxScaler
def minmax(df_df):
  global scaler
  scaler=MinMaxScaler()
  dd=scaler.fit_transform(df_df)
  return dd
data_scaled1=minmax(df_dask2)

data_scaled1

data_scaled1.shape[1]

"""
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt

n_components = (data_scaled1.shape[1])
svd = TruncatedSVD(n_components=n_components)
svd.fit_transform(data_scaled1) 
explained_var_ratio = svd.explained_variance_ratio_
plt.plot(np.cumsum(explained_var_ratio))
plt.show()
"""

from sklearn.decomposition import TruncatedSVD
svd1 = TruncatedSVD(n_components=500)
data_imputed1=svd1.fit_transform(data_scaled1)

data_imputed1.shape

svd1.components_.shape

data_imp1=np.dot(data_imputed1,svd1.components_)

data_imp1.shape

# descaling the data
final_data1=scaler.inverse_transform(data_imp1)

final_data1

# converting it into a dataframe
final_dataframe1=pd.DataFrame(final_data1,columns=columns1)

final_dataframe1.head()

X_resampled.head()

col1=set(X_resampled.columns)
col2=set(final_dataframe1.columns)

print(len(col1))
print(len(col2))

col3=col2.intersection(col1)
col4=list(col3)
len(col4)

for i in X_resampled.columns:
  if(i in col4):
    continue
  else:
    X_resampled=X_resampled.drop(i,axis=1)

for i in final_dataframe1.columns:
  if(i in col4):
    continue
  else:
    final_dataframe1=final_dataframe1.drop(i,axis=1)

final_dataframe.shape,final_dataframe1.shape

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X_resampled,y_resampled,random_state=42,test_size=0.33)

"""
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
train_score=[]
test_score=[]
values=[i for i in range(1,20)]
for i in range(1,20):
  model=RandomForestClassifier(n_estimators=100,max_depth=i,max_features='auto',random_state=None,criterion="gini")
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test)
  acc1=accuracy_score(y_pred,y_test)
  y1_pred=model.predict(x_train)
  acc=accuracy_score(y1_pred,y_train) 
  train_score.append(acc)
  test_score.append(acc1) 
plt.plot(values,train_score,'-o',label="train")
plt.plot(values,test_score,'-o',label="test")
plt.xlabel("max_depth")
plt.ylabel("acc")
plt.legend()
plt.show()
"""

# from the above graph we can conclude that after max_depth= 7 it is getting overfit

from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier(n_estimators=150,max_depth=7,max_features='auto',criterion='entropy')
model.fit(x_train,y_train)

print(len(X_resampled.columns))
print(len(final_dataframe1.columns))
if(len(X_resampled.columns)==len(final_dataframe1.columns)):
  print("same columns")
else:
  print("variable columns")

y_pred1=model.predict(final_dataframe1)

print(y_pred1)

pickle.dump(model,open('modell.pkl','wb'))



columns2=['predicted']

final_dd=pd.DataFrame(y_pred1.reshape(-1,1),columns=columns2)

final_dd['predicted'].unique()

ccc=model.predict_proba(final_dataframe1)
l=[]
for i in range(0,5000):
  l.append(ccc[i][1])

ccc

final_dd['predicted_probability']=l

final_dd.head()

final_dd.to_csv('/content/drive/MyDrive/fibe hackathon/dataset/sub.csv', index=False)

